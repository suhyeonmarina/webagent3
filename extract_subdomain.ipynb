{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31849243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique 개수: 30\n",
      "unique values: {'Sports', 'Department', 'Ground', 'Government', 'Auto', 'Home service', 'Digital', 'Game', 'Education', 'Restaurant', 'Cooking', 'Car rental', 'Health', 'Hotel', 'General', 'Finance', 'Shipping', 'Pet', 'Music', 'Speciality', 'Fashion', 'Airlines', 'Housing', 'Event', 'Movie', 'Moving', 'Social media', 'Weather', 'Other', 'Job'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"website_to_subdomain.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    online_data = json.load(f)\n",
    "\n",
    "unique_values = set(online_data.values())\n",
    "print(\"unique 개수:\", len(unique_values))\n",
    "print(\"unique values:\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a819d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to online_mind2web/Online_Mind2Web_with_subdomain.json\n",
      "Unmatched websites: 147\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# -----------------------\n",
    "# Paths\n",
    "# -----------------------\n",
    "ONLINE_PATH = \"online_mind2web/Online_Mind2Web.json\"\n",
    "MAP_PATH = \"website_to_subdomain.json\"\n",
    "OUTPUT_PATH = \"online_mind2web/Online_Mind2Web_with_subdomain.json\"\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load files\n",
    "# -----------------------\n",
    "with open(ONLINE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    online_data = json.load(f)\n",
    "\n",
    "with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    website_to_subdomain = json.load(f)\n",
    "\n",
    "# -----------------------\n",
    "# 2. website name extractor\n",
    "# -----------------------\n",
    "def extract_core_website(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract string between 'www.' and '.com'\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    match = re.search(r\"www\\.([^.]+)\\.com\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3. Attach sub_domain\n",
    "# -----------------------\n",
    "unmatched = []\n",
    "\n",
    "for item in online_data:\n",
    "    raw_website = item.get(\"website\", \"\")\n",
    "    core_name = extract_core_website(raw_website)\n",
    "\n",
    "    if core_name is None:\n",
    "        item[\"sub_domain\"] = None\n",
    "        unmatched.append(raw_website)\n",
    "        continue\n",
    "\n",
    "    # website_to_subdomain.json 은 보통 전체 URL 기준이므로\n",
    "    # key 중에 core_name 이 포함된 것을 찾음\n",
    "    matched_sd = None\n",
    "    for full_url, sd in website_to_subdomain.items():\n",
    "        if core_name in full_url:\n",
    "            matched_sd = sd\n",
    "            break\n",
    "\n",
    "    item[\"sub_domain\"] = matched_sd\n",
    "\n",
    "    if matched_sd is None:\n",
    "        unmatched.append(raw_website)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Save output\n",
    "# -----------------------\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(online_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved to {OUTPUT_PATH}\")\n",
    "print(f\"Unmatched websites: {len(unmatched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247ded62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "# Online mind2web에 처음부터 subdomain입히기\n",
    "\n",
    "INPUT_PATH = \"/mnt/raid5/parksh/Mind2web/online_mind2web/Online_Mind2Web.json\"\n",
    "OUTPUT_PATH = \"/mnt/raid5/parksh/Mind2web/online_mind2web/Online_Mind2Web_with_subdomain_llm_v2.json\"\n",
    "\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "SYSTEM_PROMPT = '''You are an expert dataset annotator.\n",
    "\n",
    "Given a website and a task description, infer the single most appropriate high-level subdomain category.\n",
    "\n",
    "Return ONLY the subdomain name as a short noun phrase.\n",
    "Do NOT include explanations, punctuation, or multiple labels.\n",
    "\n",
    "Prefer assigning a label from the following list when applicable, but you are not strictly limited to it:\n",
    "Sports, Department, Government, Auto, Home Service, Digital, Game, Education, Restaurant, Cooking,\n",
    "Car Rental, Health, Hotel, Finance, Shipping, Pet, Music, Fashion, Airlines, Housing,\n",
    "Event, Movie, Moving, Social Media, Weather, Job, Other.\n",
    "'''\n",
    "\n",
    "def build_user_prompt(website: str, task: str) -> str:\n",
    "    return f\"\"\"\n",
    "Website:\n",
    "{website}\n",
    "\n",
    "Task:\n",
    "{task}\n",
    "\n",
    "Question:\n",
    "Which subdomain does this website belong to?\n",
    "\"\"\"\n",
    "\n",
    "def infer_subdomain_with_llm(website: str, task: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": build_user_prompt(website, task)},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e159b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /mnt/raid5/parksh/Mind2web/online_mind2web/Online_Mind2Web_with_subdomain_llm_v2.json\n",
      "LLM으로 채운 row 수: 300\n"
     ]
    }
   ],
   "source": [
    "for item in data:\n",
    "    website = item.get(\"website\", \"\")\n",
    "    task = item.get(\"confirmed_task\", \"\")\n",
    "\n",
    "    try:\n",
    "        inferred_sd = infer_subdomain_with_llm(website, task)\n",
    "    except Exception as e:\n",
    "        print(\"LLM error:\", e)\n",
    "        inferred_sd = None\n",
    "\n",
    "    item[\"sub_domain\"] = inferred_sd\n",
    "    item[\"with_llm\"] = 1\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved to {OUTPUT_PATH}\")\n",
    "\n",
    "llm_count = sum(1 for x in data if x.get(\"with_llm\") == 1)\n",
    "print(\"LLM으로 채운 row 수:\", llm_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44ede09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded records: 289\n",
      "\n",
      "==============================\n",
      "Subdomain totals (desc)\n",
      "==============================\n",
      "- Health: 32 records  |  10 unique websites\n",
      "- Government: 25 records  |  11 unique websites\n",
      "- Digital: 24 records  |  12 unique websites\n",
      "- Other: 20 records  |  19 unique websites\n",
      "- Auto: 18 records  |  8 unique websites\n",
      "- Pet: 18 records  |  3 unique websites\n",
      "- Shopping: 17 records  |  13 unique websites\n",
      "- Housing: 17 records  |  6 unique websites\n",
      "- Job: 15 records  |  10 unique websites\n",
      "- Shipping: 12 records  |  4 unique websites\n",
      "- Education: 12 records  |  11 unique websites\n",
      "- Sports: 11 records  |  7 unique websites\n",
      "- Weather: 11 records  |  3 unique websites\n",
      "- Finance: 11 records  |  7 unique websites\n",
      "- Transportation: 10 records  |  8 unique websites\n",
      "- Travel: 10 records  |  7 unique websites\n",
      "- Game: 7 records  |  5 unique websites\n",
      "- Cooking: 5 records  |  3 unique websites\n",
      "- Movie: 4 records  |  3 unique websites\n",
      "- Event: 4 records  |  3 unique websites\n",
      "- Music: 3 records  |  3 unique websites\n",
      "- Social Media: 3 records  |  3 unique websites\n",
      "\n",
      "==============================\n",
      "Website counts within each subdomain\n",
      "(showing websites in desc count order)\n",
      "==============================\n",
      "\n",
      "[Health] total=32\n",
      "  - https://www.drugs.com/: 8\n",
      "  - https://www.healthgrades.com/: 6\n",
      "  - https://www.healthline.com/: 5\n",
      "  - https://www.mayoclinic.org/: 4\n",
      "  - https://www.babycenter.com/: 3\n",
      "  - https://www.webmd.com/: 2\n",
      "  - https://doctor.webmd.com/: 1\n",
      "  - https://www.thumbtack.com/: 1\n",
      "  - https://www.cvs.com/: 1\n",
      "  - https://www.americashealthrankings.org/: 1\n",
      "\n",
      "[Government] total=25\n",
      "  - https://www.recreation.gov/: 8\n",
      "  - https://www.justice.gov/: 3\n",
      "  - https://www.gov.uk/: 3\n",
      "  - https://www.dmv.virginia.gov/: 3\n",
      "  - https://new.mta.info/: 2\n",
      "  - https://ohio.gov/: 1\n",
      "  - https://medicare.gov: 1\n",
      "  - https://www.uscis.gov/: 1\n",
      "  - https://www.cbp.gov/: 1\n",
      "  - https://www.irs.gov/: 1\n",
      "  - https://www.sec.gov/: 1\n",
      "\n",
      "[Digital] total=24\n",
      "  - https://www.bestbuy.com/: 8\n",
      "  - https://www.apple.com/: 4\n",
      "  - https://www.google.com/shopping?udm=28: 3\n",
      "  - https://www.amazon.com/: 1\n",
      "  - https://www.microcenter.com/: 1\n",
      "  - https://craigslist.org/: 1\n",
      "  - https://www.nvidia.com/: 1\n",
      "  - https://www.us-appliance.com/: 1\n",
      "  - https://www.verizon.com/: 1\n",
      "  - https://versus.com/: 1\n",
      "  - https://www.samsung.com/: 1\n",
      "  - https://www.bhphotovideo.com/: 1\n",
      "\n",
      "[Other] total=20\n",
      "  - https://github.com/: 2\n",
      "  - https://www.traderjoes.com/: 1\n",
      "  - https://www.marriott.com/: 1\n",
      "  - https://www.thumbtack.com/: 1\n",
      "  - https://phys.org/: 1\n",
      "  - https://dblp.org/: 1\n",
      "  - https://www.cboe.com/: 1\n",
      "  - https://www.statista.com/: 1\n",
      "  - https://iclr.cc/: 1\n",
      "  - https://sourceforge.net/: 1\n",
      "  - https://arxiv.org/: 1\n",
      "  - https://www.americashealthrankings.org/: 1\n",
      "  - https://www.google.com/maps/: 1\n",
      "  - https://www.nvidia.com/: 1\n",
      "  - https://mega.io/: 1\n",
      "  - https://macyswineshop.com/: 1\n",
      "  - https://www.kaggle.com/: 1\n",
      "  - https://superlawyers.com/: 1\n",
      "  - https://azure.microsoft.com/: 1\n",
      "\n",
      "[Auto] total=18\n",
      "  - https://www.cars.com/: 7\n",
      "  - https://www.carmax.com/: 3\n",
      "  - https://www.kbb.com/: 2\n",
      "  - https://www.parkers.co.uk/: 2\n",
      "  - https://www.carvana.com/: 1\n",
      "  - https://www.bbb.org/: 1\n",
      "  - https://www.porsche.com/: 1\n",
      "  - https://spothero.com/: 1\n",
      "\n",
      "[Pet] total=18\n",
      "  - https://www.adoptapet.com/: 9\n",
      "  - https://www.akc.org/: 5\n",
      "  - https://www.petfinder.com/: 4\n",
      "\n",
      "[Shopping] total=17\n",
      "  - https://www.bestbuy.com/: 3\n",
      "  - https://www.google.com/shopping?udm=28: 2\n",
      "  - https://losangeles.craigslist.org/: 2\n",
      "  - https://us.speedo.com/: 1\n",
      "  - https://www.uniqlo.com/: 1\n",
      "  - https://www.target.com/: 1\n",
      "  - https://www.jcpenney.com/: 1\n",
      "  - https://raisingcanes.com/: 1\n",
      "  - https://zara.com/us: 1\n",
      "  - https://www.disneystore.com/: 1\n",
      "  - https://www.birkenstocks.com/: 1\n",
      "  - https://www.ikea.com/: 1\n",
      "  - https://www.dillards.com/: 1\n",
      "\n",
      "[Housing] total=17\n",
      "  - https://www.apartments.com/: 6\n",
      "  - https://www.landwatch.com/: 4\n",
      "  - https://www.student.com/: 3\n",
      "  - https://www.compass.com/: 2\n",
      "  - https://www.remax.com/: 1\n",
      "  - https://craigslist.org/: 1\n",
      "\n",
      "[Job] total=15\n",
      "  - https://www.thumbtack.com/: 3\n",
      "  - https://ohiomeansjobs.ohio.gov/: 3\n",
      "  - https://www.amazon.jobs/: 2\n",
      "  - https://corporate.target.com/: 1\n",
      "  - https://careers.walmart.com/: 1\n",
      "  - https://www.mayoclinic.org/: 1\n",
      "  - https://jobs.chronicle.com: 1\n",
      "  - https://www.ca.gov/: 1\n",
      "  - https://ziprecruiter.com/: 1\n",
      "  - https://www.ycombinator.com/: 1\n",
      "\n",
      "[Shipping] total=12\n",
      "  - https://www.ups.com/: 5\n",
      "  - https://www.fedex.com/en-us/home.html: 4\n",
      "  - https://www.usps.com/: 2\n",
      "  - https://www.publicstorage.com/: 1\n",
      "\n",
      "[Education] total=12\n",
      "  - https://www.coursera.org/: 2\n",
      "  - https://umich.edu/: 1\n",
      "  - https://www.osu.edu/: 1\n",
      "  - https://www.berkeley.edu/: 1\n",
      "  - https://www.stanford.edu/: 1\n",
      "  - https://www.instructure.com/: 1\n",
      "  - https://www.merriam-webster.com/: 1\n",
      "  - https://www.ted.com/: 1\n",
      "  - https://www.youtube.com/: 1\n",
      "  - https://phet.colorado.edu/: 1\n",
      "  - https://www.coolmath4kids.com/: 1\n",
      "\n",
      "[Sports] total=11\n",
      "  - https://www.nba.com/: 4\n",
      "  - https://www.espn.com/: 2\n",
      "  - https://www.foxsports.com/: 1\n",
      "  - https://www.nfl.com/: 1\n",
      "  - https://www.stubhub.com/: 1\n",
      "  - https://www.redbull.com/: 1\n",
      "  - https://www.backcountry.com/: 1\n",
      "\n",
      "[Weather] total=11\n",
      "  - https://www.accuweather.com/: 6\n",
      "  - https://www.theweathernetwork.com/: 4\n",
      "  - https://weather.com/: 1\n",
      "\n",
      "[Finance] total=11\n",
      "  - https://www.google.com/finance/: 3\n",
      "  - https://finance.yahoo.com/: 2\n",
      "  - https://www.chase.com/: 2\n",
      "  - https://coinmarketcap.com/: 1\n",
      "  - https://www.nyse.com/: 1\n",
      "  - https://smartasset.com/: 1\n",
      "  - https://www.americanexpress.com/: 1\n",
      "\n",
      "[Transportation] total=10\n",
      "  - https://www.flightaware.com/: 3\n",
      "  - https://www.qatarairways.com/: 1\n",
      "  - https://www.amtrak.com/: 1\n",
      "  - https://www.united.com/: 1\n",
      "  - https://us.megabus.com/: 1\n",
      "  - https://www.carnival.com/: 1\n",
      "  - https://new.mta.info/: 1\n",
      "  - https://www.ryanair.com/: 1\n",
      "\n",
      "[Travel] total=10\n",
      "  - https://us.trip.com/: 2\n",
      "  - https://www.apartments.com/: 2\n",
      "  - https://www.airbnb.com/: 2\n",
      "  - https://www.booking.com/: 1\n",
      "  - https://www.google.com/maps/: 1\n",
      "  - https://wanderlog.com/: 1\n",
      "  - https://www.tourradar.com/: 1\n",
      "\n",
      "[Game] total=7\n",
      "  - https://www.ign.com/: 2\n",
      "  - https://store.steampowered.com/: 2\n",
      "  - https://www.leagueoflegends.com/: 1\n",
      "  - https://www.chess.com/: 1\n",
      "  - https://boardgamegeek.com/: 1\n",
      "\n",
      "[Cooking] total=5\n",
      "  - https://cookpad.com/: 3\n",
      "  - https://www.allrecipes.com/: 1\n",
      "  - https://www.healthline.com/: 1\n",
      "\n",
      "[Movie] total=4\n",
      "  - https://www.imdb.com/: 2\n",
      "  - https://www.rottentomatoes.com/: 1\n",
      "  - https://www.fandom.com/: 1\n",
      "\n",
      "[Event] total=4\n",
      "  - https://www.stubhub.com/: 2\n",
      "  - https://www.eventbrite.com/: 1\n",
      "  - https://ticketmaster.com/: 1\n",
      "\n",
      "[Music] total=3\n",
      "  - https://www.discogs.com/: 1\n",
      "  - https://soundcloud.com/: 1\n",
      "  - https://bandcamp.com/: 1\n",
      "\n",
      "[Social Media] total=3\n",
      "  - https://www.tumblr.com/: 1\n",
      "  - https://9gag.com/: 1\n",
      "  - https://imgur.com/: 1\n",
      "\n",
      "==============================\n",
      "Missing key stats\n",
      "==============================\n",
      "Missing sub_domain: 0\n",
      "Missing website:    0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = \"online_mind2web/Online_Mind2Web_with_subdomain_llm_v2.json\"\n",
    "\n",
    "def load_json_or_jsonl(path: str):\n",
    "    \"\"\"JSON(list/dict) 또는 JSONL(list of json objects) 모두 로드\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    # 1) JSON로 먼저 시도\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # 흔한 케이스: list[dict]\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        # dict 형태면 내부에 list가 들어있을 수도 있으니 최대한 안전하게 처리\n",
    "        if isinstance(data, dict):\n",
    "            # \"data\" / \"items\" 같은 키에 리스트가 있는 경우를 우선 처리\n",
    "            for k in [\"data\", \"items\", \"examples\", \"records\"]:\n",
    "                if k in data and isinstance(data[k], list):\n",
    "                    return data[k]\n",
    "            # 아니면 dict의 value들 중 dict들이면 flatten 시도(비권장이지만 안전망)\n",
    "            vals = list(data.values())\n",
    "            if vals and all(isinstance(v, dict) for v in vals):\n",
    "                return vals\n",
    "            raise ValueError(\"Loaded JSON is a dict, but cannot find a list of records inside.\")\n",
    "    except json.JSONDecodeError:\n",
    "        pass  # JSON 실패 -> JSONL 시도\n",
    "\n",
    "\n",
    "def get_key(record: dict, candidates):\n",
    "    \"\"\"여러 후보 키 중 존재하는 첫 키를 반환\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in record:\n",
    "            return record.get(k)\n",
    "    return None\n",
    "\n",
    "def normalize(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, str):\n",
    "        v = v.strip()\n",
    "        return v if v else None\n",
    "    return v\n",
    "\n",
    "def main():\n",
    "    data = load_json_or_jsonl(FILE_PATH)\n",
    "    print(f\"Loaded records: {len(data)}\")\n",
    "\n",
    "    # subdomain -> website counter\n",
    "    subdomain_to_websites = defaultdict(Counter)\n",
    "\n",
    "    missing_sd = 0\n",
    "    missing_web = 0\n",
    "\n",
    "    for rec in data:\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "\n",
    "        sd = normalize(get_key(rec, [\"sub_domain\", \"subdomain\", \"subDomain\"]))\n",
    "        web = normalize(get_key(rec, [\"website\", \"site\", \"domain\"]))\n",
    "\n",
    "        if sd is None:\n",
    "            missing_sd += 1\n",
    "            sd = \"__NULL_SUBDOMAIN__\"\n",
    "        if web is None:\n",
    "            missing_web += 1\n",
    "            web = \"__NULL_WEBSITE__\"\n",
    "\n",
    "        subdomain_to_websites[sd][web] += 1\n",
    "\n",
    "    # 1) subdomain별 전체 개수\n",
    "    subdomain_total = {sd: sum(counter.values()) for sd, counter in subdomain_to_websites.items()}\n",
    "    # 총합 기준 내림차순 정렬\n",
    "    sorted_subdomains = sorted(subdomain_total.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Subdomain totals (desc)\")\n",
    "    print(\"==============================\")\n",
    "    for sd, total in sorted_subdomains:\n",
    "        n_websites = len(subdomain_to_websites[sd])\n",
    "        print(f\"- {sd}: {total} records  |  {n_websites} unique websites\")\n",
    "\n",
    "    # 2) 각 subdomain 안에서 website별 개수\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Website counts within each subdomain\")\n",
    "    print(\"(showing websites in desc count order)\")\n",
    "    print(\"==============================\")\n",
    "    for sd, total in sorted_subdomains:\n",
    "        print(f\"\\n[{sd}] total={total}\")\n",
    "        web_counter = subdomain_to_websites[sd]\n",
    "        for web, cnt in web_counter.most_common():\n",
    "            print(f\"  - {web}: {cnt}\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Missing key stats\")\n",
    "    print(\"==============================\")\n",
    "    print(f\"Missing sub_domain: {missing_sd}\")\n",
    "    print(f\"Missing website:    {missing_web}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto + transportation = transportation\n",
    "# digital + shopping = shopping\n",
    "# game + movie + event + music + social media = entertainment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webvoyager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
